{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time Series Data Analysis\n",
    "\n",
    "## Time Series with Pandas\n",
    "\n",
    "**DateTime Index**\n",
    "\n",
    "To import the libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the **datetime()** into a variable. The order of the time is shown below. Note that the variables year, month, day, ... must be defined earlier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_date = datetime(year,month,day,hour,minute,second)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The type fo the data now is **datetime.datetime**. We can grab any part of the datetime object that we want using my_date.[...]. Examples are shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_date.hour # shows the hour of my_date variable\n",
    "my_date.year # gives the year of my_date variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "**Numpy Datetime Arrays**\n",
    "\n",
    "The NumyPy data type that handles dates is called **datetime64**. The bracket of the dtype shows the level of precision. It can be [Y], [M], [D], [h], [m], [s]. An example of using that is as below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(['2020-03-15', '2020-05-24', '2020-08-09'],dtype='datetime64[D]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also can use the numpy **arange** function to generate an aray of dates:\n",
    "\n",
    "np.arange(start_date,end_date,step,dtype='datetime64[D]')\n",
    "\n",
    "The bracket of the dtype shows that the step should be applied on the days. We can see an example below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.arange('2018-06-01','2018-06-23', 7, dtype='datetime64[D]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "**Pandas Date Ranges**\n",
    "\n",
    "Here we learn how to convert incoming text to date using pandas. \n",
    "\n",
    "The following are the three possible ways building a **DatetimeIndex**. \n",
    "\n",
    "  1. Using **pd.date_range()**: In this method we first start with a date i.e. '7-8-2020'. Then we need to enter the period wich is the total numbers of the date that we need i.e. periods=7, and we need to enter frequency as freq='D'. Here 'D' means that frequency should be applied as date.\n",
    "  \n",
    "  2. Using **pd.to_datetime()**: In this method we pass a list of dates(text) to the function and it will be converted to datetime.\n",
    "  \n",
    "  3. Using **pd.DatetimeIndex()**: In this method we pass a list of dates to the function and also specify the datatype. In the first step we generate a numpy array and its specified **datetime64[...]** datatype(dtype). Then, we pass the np.array to the function \n",
    "  \n",
    "**Note** As it can be infered, pd.date_range is useful when we want to generate a **Series of dates**. pd.to_datetime is useful for converting text format of datetime column to numbers, and pd.DateTimeIndex is useful for the case of converting a numpy array to pandas Series.   \n",
    "  \n",
    "The examples of each method are as following:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using pd.date_range()\n",
    "pd.date_range('07-08-2020', periods=7, freq='D') # generates a list including 7 dates starting from the enetered date and add to Day(D)\n",
    "\n",
    "# Using pd.to_datetime()\n",
    "pd.to_datetime(['07-08-2020','07-09-2020','07-09-2020'])\n",
    "\n",
    "# Using pd.DatetimeIndex()\n",
    "some_dates = np.array(['2016-03-15', '2017-05-24', '2018-08-09'], dtype='datetime64[D]') # first we create a NumPy array\n",
    "\n",
    "pd.DatetimeIndex(some_dates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time Resampling\n",
    "\n",
    "We can think of time resampling as very similar to a groupby function but aggregating based off some sort of time frequency. For example we can take daily data and resample it to monthly data by taking average over month or sum per month. \n",
    "\n",
    "**Note** Sometimes we need to read the date column as index of a DataFrame. To this end the reading file command is as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('file_path',index_col='Date_col', parse_dates=True)\n",
    "# index_col determines which column can be index\n",
    "# Note that if we set parse_dates=Falsse, then the date column will\n",
    "# be a normal index but True makes it DatetimeIndex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A common operation with time series data is resampling based on the time series index. When calling **.resample()** first we need to pass in a **rule** parameter, then we need to call some sort of aggregation function. The rule parameter describes the frequency with which to apply the aggregation function.  The following is the link aliases used for rule. [[reference](http://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#offset-aliases)]\n",
    "\n",
    "**code** of resampling data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.resample(rule='A').mean()\n",
    "\n",
    "# .mean() is the aggregate function\n",
    "#'A' specifies the "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the well known aggregate functions are min, max, sum, mean, std\n",
    "\n",
    "We also can define our own function to be applied on the resampler using .apply().\n",
    "\n",
    "### Time Shifting \n",
    "\n",
    "Sometimes we need to shift all our data up or down along the TimeSeriesIndex and pandas has a built-in methods for this. **code** \n",
    "\n",
    "the first line of the code shifts all columns of data frame one row down and the second line does the same one row up. As a result of the first line of the following code the first row of the data frame will be null and we loose the information of the last row and vise versa for the second line of the code. \n",
    "\n",
    "The third line of the code determines a frequency for shifting. for example 'M' specifies shifting based off the month.\n",
    "\n",
    "**Note:** .shift() without frequency shifts all columns except TimeSeriesIndex. If we specif the freq, then we can shift the TimeSeriesDataIndex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shift(1)   # shift down\n",
    "df.shift(-1)  # shift up\n",
    "df.shift(1, freq='M')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rolling and Expanding\n",
    "\n",
    "**Rolling**\n",
    "\n",
    "The basic premise is that a common process with time series is create data based off a rolling meme. So what we can do is divide the data into windows of time then calculate an aggregate function for each window. Let's show how we can do this in pandas.  \n",
    "\n",
    "**code**\n",
    "\n",
    "In the first line of the code window=7 shows that we have chosen every 7 rows to calculate the average. The second and third column of data, plot the original and the rolled data to compare them. **Note:** To have legend for the plot we can first save the rolling as a column in the DataFrame and then plot both columns. Line 4 and 5 of the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rolloing(window=7).mean()\n",
    "\n",
    "df['col_name'].plot()\n",
    "\n",
    "df.rolling(window=7).mean()['col_name'].plot()\n",
    "\n",
    "df['col_name_rolled'] = df['col_name'].rolling(window=7).mean\n",
    "\n",
    "df[['col_name','col_name_rolled']].plot()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expanding**\n",
    "\n",
    "So instead of calculating values for a rolling when those dates have what we wanted to take into account everything from the start of the TimeSeries up to each point in time. For example instead of considering the average over the last seven days we would consider all prior to data in our expanding set of averages. **Code**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['col_name'].expanding().mean()\n",
    "df['col_name'].expanding().mean().plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Time Series Data\n",
    "\n",
    "To plot all the columns of a DataFrame vs. index column we use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot()   # Plots all the columns in one frame.\n",
    "            # Note that if the columns have different scales then \n",
    "            # we should plot each column separately.\n",
    "        \n",
    "df['col_name'].plot() # plots col_name column of the dataframe vs.\n",
    "                      # index column.\n",
    "    \n",
    "# for plotting more than one column we have\n",
    "\n",
    "df[['col_name1','col_name2']].plot()  \n",
    "\n",
    "\n",
    "# To set title, xlabel, ylabel\n",
    "# autoscal will scale the x and y axis. axis={'x, y, or ,both'} \n",
    "# and tight determines if we need blank space at the begining or \n",
    "# end of data set for both axis\n",
    "\n",
    "ax = df['col_name'].plot(figsize=(12,6), title=title)\n",
    "ax.autoscale(axis='both',tight=False)\n",
    "ax.set(xlabel='XLABEL',ylabel='YLABEL')\n",
    "\n",
    "# set x or y limits\n",
    "\n",
    "df['col_name'].plot(figsize=(), title=, xlim=[beg_point, end_point]\n",
    "                   ylim=[beg_point, end_point])\n",
    "\n",
    "# To specify the linestyle we use ls='--' and for color we use \n",
    "# c='green'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we focus on the visualizations particular for daytime index objects. \n",
    "\n",
    "We start off by setting the spacing and formatting for xticks. To this end we need the following block of code:\n",
    "\n",
    "Link to all of the <tt>matplotlib<tt> locators <a href='https://matplotlib.org/api/dates_api.html#date-tickers'>https://matplotlib.org/api/dates_api.html#date-tickers</a>\n",
    "    \n",
    "Locator sets the location of the xticx and formatter sets the format of xticks.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import dates\n",
    "\n",
    "ax = df['col_name'].plot(figsize=(12,6))\n",
    "ax.xaxis.set_major_locator(dates.WeekdayLocator(byweekday=0))\n",
    "ax.xaxis.set_major_formatter(dates.DateFormatter(\"%a-%B-%d\"))\n",
    "\n",
    "ax.xaxis.set_minor_locator(dates.MonthLocator())\n",
    "ax.xaxix.set_minor_formatter(dates.DateFormatter('\\n\\n%b'))\n",
    "\n",
    "# set grid\n",
    "\n",
    "ax.yaxis.grid(True)\n",
    "ax.xaxis.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time Series analysis with statmodels\n",
    "\n",
    "Statsmodels is a python module that provides classes and functions for the estimation of many different statistical models as well as conducting statistical test and statistical data exploration.\n",
    "\n",
    "\n",
    "### Import a function test from statmodels library\n",
    "\n",
    "**Components of Time Series**\n",
    "\n",
    "\n",
    "1. **Trends** can be considered as the slope of a plot (plot of a variable vs. time). It can be **upward** (positive slope), **downward** (negative slope), and horizontal/stationary (zero slope).\n",
    "\n",
    "2. **Seasonality** it's a repeating trend.\n",
    "\n",
    "3. **cyclical** trends with no set repetition over a prticular period.\n",
    "\n",
    "**Heidrick Presscott filter**\n",
    "\n",
    "The Heidrick Prescott filter separates time series $y_t$ into trend component $\\tau_t$ and a cyclical component $c_t$. So,\n",
    "\n",
    "$y_t = \\tau_t+c_t$\n",
    "\n",
    "The compoents of the above function (trends and cyclic) can be found by minimizing the following quadratic loss function\n",
    "\n",
    "$min \\sum_{t=1}^T c^2_t +\\lambda \\sum_{t=1}^T [(\\tau_t-\\tau_{t-1})-(\\tau_{t-1}-\\tau_{t-2})]^2$\n",
    "\n",
    "where $\\lambda$ is a **smoothing** parameter. The $\\lambda$ value above handles variations in the growth rate of the trend component. The above function is actually the function that Heidrick Perscott filter uses to separate the components. \n",
    "\n",
    "**Note:** When we are analyzing **quarterly data**, the default lambda value of 1600 is recomended and 6.25 for annual data and 129,600 for monthly data.  \n",
    "\n",
    "**codes of Heidrick Prescott filter**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.filters import hpfilter\n",
    "\n",
    "cycle_comp, trend_com = hpfilter(df['col_name'], lamb=1600)\n",
    "\n",
    "'''\n",
    "Note that hpfilter() result is a tuple so we have unpacked \n",
    "it into cycle component and trend component. lamb=1600 for \n",
    "quarterly data.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error-Trend-Seasonality (ETS) model\n",
    "\n",
    "1. Exponential Smoothing\n",
    "\n",
    "2. Trend Methods models \n",
    "\n",
    "3. ETS Decomposition\n",
    "\n",
    "\n",
    "There can be a **seasonality component** a **Trend Component** and **Error Component** with time series.\n",
    "\n",
    "In this section we are going to try to explore what these actual components E T and S are. Then, we visulize the data based off its ETS which is a good way to build and understanding of its behaviour.\n",
    "\n",
    "**Note** We have two kinds of trends: **Linear** and **Exponential**\n",
    "\n",
    "When we use **ETS Decomposition** it returns 4 plots:\n",
    "\n",
    "   1. Observed: Original Raw Data\n",
    "   2. Trend: Trend of the Data\n",
    "   3. Seasonality: Periods of repition\n",
    "   4. Residual: Whatever is not explained by Trend or Seasonality.\n",
    "   \n",
    "We two main test models when we do ETS decomposition.\n",
    "\n",
    "   1. Addditive model: We apply and additive model when the trends is more linear and the seasonality and trend components seem to be constant over time\n",
    "   2. multiplicative model: It is more appropriate when we are increasing or decreasing at a non-linear rate.\n",
    "\n",
    "Now let's explore how to performe an ETS decomposition with statsmodels.\n",
    "\n",
    "**Code**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Note: when we do ETS we should drop null values\n",
    "'''\n",
    "\n",
    "df = pd.read_csv('file_path', index_col='col_name', parse_dates=True)\n",
    "\n",
    "df = df.dropna()\n",
    "'''\n",
    "To Choose additive or multiplicative models we look at the \n",
    "trend. If it's linear we use additive and if its nonlinear\n",
    "we use multiplicative\n",
    "'''\n",
    "\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "\n",
    "result = seasonal_decompose(df['col_name'],model='multiplicative')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Exponentially Weighted Moving Average Models (EWMA\n",
    "\n",
    "**Weighted Moving Average (WMA)** Allows us to create simple model that describes some **trend level behavior** of a time series. We could theoretically attempt to use the simple weighted moving averages to build a generalized model for the real word time series we are analyzing. \n",
    "\n",
    "In this section we are trying to create a generalized model to describe the general behavior of our time series. An issue with the **Simple Moving Average (SMA)** is that the entire model will be constrained to the same window size. \n",
    "\n",
    "Some SMA weaknesses:\n",
    "\n",
    " 1. Smaller windows will lead to more noise, rather than signal\n",
    " 2. It will never reach to full peak or valley of the data due to the averaging. \n",
    " 3. Does not really inform us about possible future behavior. All it does is describe trends in our data.\n",
    " 4. Extreme historical values can skew our SMA significantly.\n",
    " \n",
    "**EWMA** will allow us to reduce the lag effect from SMA and it will put more weight on values that occured more recently (it exponentially wieghts the values). The amount of weight applied to the most recent values will depend on the actual parameters used in the EWMA and the number of periods given a window size \n",
    " \n",
    "**EWMA Code Along**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We should first drop the null values\n",
    "\n",
    "# Read file\n",
    "\n",
    "df = pd.read_csv('file_path', index_col='col_name')\n",
    "\n",
    "df.dropna(inplace=True)\n",
    "df.index= pd.to_datetime(df.index)\n",
    "\n",
    "df['col_name'].ewm(span=12).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Mathematics Behind EWMA**\n",
    "\n",
    "The formula of EWMA is:\n",
    "\n",
    "### $y_t =   \\frac{\\sum\\limits_{i=0}^t w_i x_{t-i}}{\\sum\\limits_{i=0}^t w_i}$\n",
    "\n",
    "Where $x_t$ is the input value, $w_i$ is the applied weight (Note how it can change from $i=0$ to $t$), and $y_t$ is the output.\n",
    "\n",
    "Now the question is, how to we define the weight term $w_i$?\n",
    "\n",
    "This depends on the <tt>adjust</tt> parameter you provide to the <tt>.ewm()</tt> method.\n",
    "\n",
    "When <tt>adjust=True</tt> (default) is used, weighted averages are calculated using weights equal to $w_i = (1 - \\alpha)^i$\n",
    "\n",
    "which gives\n",
    "\n",
    "### $y_t = \\frac{x_t + (1 - \\alpha)x_{t-1} + (1 - \\alpha)^2 x_{t-2} + ...\n",
    "+ (1 - \\alpha)^t x_{0}}{1 + (1 - \\alpha) + (1 - \\alpha)^2 + ...\n",
    "+ (1 - \\alpha)^t}$\n",
    "\n",
    "When <tt>adjust=False</tt> is specified, moving averages are calculated as:\n",
    "\n",
    "### $\\begin{split}y_0 &= x_0 \\\\\n",
    "y_t &= (1 - \\alpha) y_{t-1} + \\alpha x_t,\\end{split}$\n",
    "\n",
    "which is equivalent to using weights:\n",
    "\n",
    " \\begin{split}w_i = \\begin{cases}\n",
    "    \\alpha (1 - \\alpha)^i & \\text{if } i < t \\\\\n",
    "    (1 - \\alpha)^i        & \\text{if } i = t.\n",
    "\\end{cases}\\end{split}\n",
    "\n",
    "When <tt>adjust=True</tt> we have $y_0=x_0$ and from the last representation above we have \n",
    "$y_t=\\alpha x_t+(1−α)y_{t−1}$, therefore there is an assumption that $x_0$ is not an ordinary value but rather an exponentially weighted moment of the infinite series up to that point.\n",
    "\n",
    "For the smoothing factor $\\alpha$ one must have $0<\\alpha≤1$, and while it is possible to pass <em>alpha</em> directly, it’s often easier to think about either the <em>span</em>, <em>center of mass</em> (com) or <em>half-life</em> of an EW moment:\n",
    "\n",
    "\\begin{split}\\alpha =\n",
    " \\begin{cases}\n",
    "     \\frac{2}{s + 1},               & \\text{for span}\\ s \\geq 1\\\\\n",
    "     \\frac{1}{1 + c},               & \\text{for center of mass}\\ c \\geq 0\\\\\n",
    "     1 - \\exp^{\\frac{\\log 0.5}{h}}, & \\text{for half-life}\\ h > 0\n",
    " \\end{cases}\\end{split}\n",
    " \n",
    "* <strong>Span</strong> corresponds to what is commonly called an “N-day EW moving average”.\n",
    "* <strong>Center of mass</strong> has a more physical interpretation and can be thought of in terms of span: $c=(s−1)/2$\n",
    "* <strong>Half-life</strong> is the period of time for the exponential weight to reduce to one half.\n",
    "* <strong>Alpha</strong> specifies the smoothing factor directly.\n",
    "\n",
    "We have to pass precisely one of the above into the <tt>.ewm()</tt> function. For our data we'll use <tt>span=12</tt>. \n",
    "\n",
    "### Holt Winters Method\n",
    "\n",
    "Previously we applied simple EWMA using just one **smoothing factor $\\alpha$**. This fails to account for other contribuiting factors like trend and seasonality.\n",
    "\n",
    "In this section we focus on fitting the Holt-Winters model on a time series data set. \n",
    "\n",
    "**Theory of Holt-Winter**\n",
    "\n",
    "The Holt-Winters seasonal method comprises of the forecast equation and three additional smoothing equation. The smoothing equations are for \n",
    "\n",
    " 1. level $l_t$: the smoothing parameter is $\\alpha$\n",
    " 2. trend $b_t$: the smoothing parameter is $\\beta$\n",
    " 3. seasonal $s_t$: the smoothing parameter is $\\gamma$\n",
    "\n",
    "There are two variations to this method that differ in the nature of the **seasonal component**. First is **additive method** for the case when the seasonal variation is **constant** through the series and the second is **multiplicative** when the seasonal variations are changing proportional to the level of the series. In a nutshell for the **Holt-Winters** seasonal we have two methods\n",
    "\n",
    "  1. Additive\n",
    "  2. Multiplicative\n",
    "  \n",
    "The Holt-winter method allows us to add **double and triple exponential** smoothing.\n",
    "\n",
    "In <strong>Double Exponential Smoothing</strong> (aka Holt's Method) we introduce a new smoothing factor $\\beta$ (beta) that addresses trend:\n",
    "\n",
    "\\begin{split}l_t &= (1 - \\alpha) l_{t-1} + \\alpha x_t, & \\text{    level}\\\\\n",
    "b_t &= (1-\\beta)b_{t-1} + \\beta(l_t-l_{t-1}) & \\text{    trend}\\\\\n",
    "y_t &= l_t + b_t & \\text{    fitted model}\\\\\n",
    "\\hat y_{t+h} &= l_t + hb_t & \\text{    forecasting model (} h = \\text{# periods into the future)}\\end{split}\n",
    "\n",
    "Because we haven't yet considered seasonal fluctuations, the forecasting model is simply a straight sloped line extending from the most recent data point. We'll see an example of this in upcoming lectures.\n",
    "\n",
    "With <strong>Triple Exponential Smoothing</strong> (aka the Holt-Winters Method) we introduce a smoothing factor $\\gamma$ (gamma) that addresses seasonality:\n",
    "\n",
    "\\begin{split}l_t &= (1 - \\alpha) l_{t-1} + \\alpha x_t, & \\text{    level}\\\\\n",
    "b_t &= (1-\\beta)b_{t-1} + \\beta(l_t-l_{t-1}) & \\text{    trend}\\\\\n",
    "c_t &= (1-\\gamma)c_{t-L} + \\gamma(x_t-l_{t-1}-b_{t-1}) & \\text{    seasonal}\\\\\n",
    "y_t &= (l_t + b_t) c_t & \\text{    fitted model}\\\\\n",
    "\\hat y_{t+m} &= (l_t + mb_t)c_{t-L+1+(m-1)modL} & \\text{    forecasting model (} m = \\text{# periods into the future)}\\end{split}\n",
    "\n",
    "Here $L$ represents the number of divisions per cycle. In our case looking at monthly data that displays a repeating pattern each year, we would use $L=12$.\n",
    "\n",
    "In general, higher values for $\\alpha$, $\\beta$ and $\\gamma$ (values closer to 1), place more emphasis on recent data.\n",
    "\n",
    "**Code Along Holt-Winters method:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "After reading the timeseries it is useful to first check\n",
    "the frequency of data. if freq=None we should change it\n",
    "according to the dataset. An example is shown below\n",
    "'''\n",
    "df.index.freq = 'MS'\n",
    "\n",
    "# We should check online for more aliases\n",
    "\n",
    "from statsmodels.tsa.holtwinters import SimpleExpSmoothing\n",
    "\n",
    "span =12 #monthly\n",
    "alpha = 2/(span+1)\n",
    "\n",
    "df['EWMA'] = df['col_name'].ewm(alpha=alpha, adjust=False).mean()\n",
    "\n",
    "# The above line calculates the simple exp smoothing and\n",
    "# saves into a new column called EWMA\n",
    "# The above line can be written also by using \n",
    "# SimpleExpSmoothing code as well\n",
    "\n",
    "model = SimpleExpSmoothing(df['col_name'])\n",
    "fittet_model = model.fit(smoothing_level=alpha, omptimized=False)\n",
    "df['SES12'] = fitted_model.fittedvalues.shift(-1)\n",
    "\n",
    "# We need .shift(-1) since we have used optimizer = False\n",
    "\n",
    "\n",
    "##****** Exponential (double and triple) smoothing\n",
    "\n",
    "#import lib\n",
    "\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "\n",
    "\n",
    "# Writing everything in one line for DOUBLE EXPONENTIAL SMOOTHING\n",
    "\n",
    "df['DES-add-12'] = ExponentialSmoothing(df['col_name'],trend='add').fit().fittedvalues.shift(-1)\n",
    "\n",
    "df['DES-mul-12'] = ExponentialSmoothing(df['col_name'],trend='mul').fit().fittedvalues.shift(-1)\n",
    "\n",
    "# trend = 'add' specifies if the trend is linear\n",
    "# trend = 'mul' specifies if the trend is multiplication\n",
    "\n",
    "\n",
    "# Writing everything in one line for DOUBLE EXPONENTIAL SMOOTHING\n",
    "\n",
    "df['TES-add-12'] = ExponentialSmoothing(df['col_name'],trend='add',seasonal='add', seasonal_periods=12).fit().fittedvalues\n",
    "\n",
    "df['TES-mul-12'] = ExponentialSmoothing(df['col_name'],trend='mul',seasonal='mul', seasonal_periods=12).fit().fittedvalues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General Forecasting procedure\n",
    "\n",
    "  1. Choose a Model\n",
    "  2. Split data into train and test sets\n",
    "  3. Fit model on training set \n",
    "  4. Evaluate model on test set\n",
    "  5. Re-fit model on entire data set\n",
    "  6. Forecast for future data\n",
    "  \n",
    "  \n",
    "### Evaluating Forecast predictions\n",
    "\n",
    "In Forecasting model the **test set** will be the **most recent** end of the data. The question is now what the portion of the test data should be? There is no an exact answer for this question. Typically the size of the test data is about 20% of the total sample. It depends on the time length of the data and how far we want to forecast. \n",
    "\n",
    "The test set should ideally be at least as large as the maximum forecast horizon required. It should be noted that the longer the forecast horizon, the more likely our prediction becomes less accurate.\n",
    "\n",
    "**Code for train test split**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = df.iloc[:109]  \n",
    "test_data = df.iloc[108:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we use Holt-Winters model to forecast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.holt-winters import ExponentialSmoothing\n",
    "\n",
    "fitted_model = ExponentiaSmoothing(train_data['col_name'],trend='mul',seasonal='mul'\n",
    "                                  , seasonal_periods=12).fit()\n",
    "\n",
    "test_predictions = fitted_model.forecast(36)\n",
    "#36 is the number of periods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation metrics for forecasting\n",
    "\n",
    "Some of the most common evaluation metrics for regression probelm are:\n",
    "\n",
    "  1. Mean Absolute Error\n",
    "  2. Mean Squared Error\n",
    "  3. Root Mean Squared Error\n",
    "  \n",
    "Whenever we perform a forecast for a continuous value on a test set, we have two values:\n",
    "\n",
    "   1. y: the real value of test data\n",
    "   2. $\\hat{y}$: the predicted value from our forecast\n",
    "   \n",
    "Then we calculate the MAE, MSE, RMSE metrics based on these two values.\n",
    "\n",
    "**Forecast for future dates**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we should again train model on the whole dataset\n",
    "\n",
    "final_model = ExponentialSmoothing(df['col_name'],\n",
    "                                  trend='mul',\n",
    "                                  seasonal='mul',\n",
    "                                  seasonal_periods=12).fit()\n",
    "\n",
    "forecast_predictions = final_model.forecast(36)\n",
    "\n",
    "#forecasting for the next 3 years"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Stationarity and differencing**\n",
    "\n",
    "Stationarity occurs if the data **does not exhibit trends or seasonality** that is fluctuation in data is entirely due to **outside forces**. Stationary data shows neither trend nor seasonality. Non-stationary data shows either trends or seasonality.\n",
    "\n",
    "**Code for calculating differencing in timeseries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First way\n",
    "\n",
    "df['col_name']-df['col_name'].shift(1)\n",
    "\n",
    "'''\n",
    "So it's just taking a time series and calculating its difference \n",
    "with that time series shifted by an order which is equal to \n",
    "1 here.\n",
    "'''\n",
    "\n",
    "# Second Way\n",
    "\n",
    "from statmodels.tsa.statespace.tools import diff\n",
    "\n",
    "diff(df['col_name'],k_diff=1)\n",
    "\n",
    "# here k_diff scpecifies the order of difference\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autocovariance for 1D\n",
    "In a <em>deterministic</em> process, like $y=sin(x)$, we always know the value of $y$ for a given value of $x$. However, in a <em>stochastic</em> process there is always some randomness that prevents us from knowing the value of $y$. Instead, we analyze the past (or <em>lagged</em>) behavior of the system to derive a probabilistic estimate for $\\hat{y}$.\n",
    "\n",
    "One useful descriptor is <em>covariance</em>. When talking about dependent and independent $x$ and $y$ variables, covariance describes how the variance in $x$ relates to the variance in $y$. Here the size of the covariance isn't really important, as $x$ and $y$ may have very different scales. However, if the covariance is positive it means that $x$ and $y$ are changing in the same direction, and may be related.\n",
    "\n",
    "With a time series, $x$ is a fixed interval. Here we want to look at the variance of $y_t$ against lagged or shifted values of $y_{t+k}$\n",
    "\n",
    "For a stationary time series, the autocovariance function for $\\gamma$ (gamma) is given as:\n",
    "\n",
    "${\\displaystyle {\\gamma}_{XX}(t_{1},t_{2})=\\operatorname {Cov} \\left[X_{t_{1}},X_{t_{2}}\\right]=\\operatorname {E} [(X_{t_{1}}-\\mu _{t_{1}})(X_{t_{2}}-\\mu _{t_{2}})]}$\n",
    "\n",
    "We can calculate a specific $\\gamma_k$ with:\n",
    "\n",
    "${\\displaystyle \\gamma_k = \\frac 1 n \\sum\\limits_{t=1}^{n-k} (y_t - \\bar{y})(y_{t+k}-\\bar{y})}$\n",
    "\n",
    "### Autocovariance Example:\n",
    "Say we have a time series with five observations: {13, 5, 11, 12, 9}.<br>\n",
    "We can quickly see that $n = 5$, the mean $\\bar{y} = 10$, and we'll see that the variance $\\sigma^2 = 8$.<br>\n",
    "The following calculations give us our covariance values:\n",
    "<br><br>\n",
    "$\\gamma_0 = \\frac {(13-10)(13-10)+(5-10)(5-10)+(11-10)(11-10)+(12-10)(12-10)+(9-10)(9-10)} 5 = \\frac {40} 5 = 8.0 \\\\\n",
    "\\gamma_1 = \\frac {(13-10)(5-10)+(5-10)(11-10)+(11-10)(12-10)+(12-10)(9-10)} 5 = \\frac {-20} 5 = -4.0 \\\\\n",
    "\\gamma_2 = \\frac {(13-10)(11-10)+(5-10)(12-10)+(11-10)(9-10)} 5 = \\frac {-8} 5 = -1.6 \\\\\n",
    "\\gamma_3 = \\frac {(13-10)(12-10)+(5-10)(9-10)} 5 = \\frac {11} 5 = 2.2 \\\\\n",
    "\\gamma_4 = \\frac {(13-10)(9-10)} 5 = \\frac {-3} 5 = -0.6$\n",
    "<br><br>\n",
    "Note that $\\gamma_0$ is just the population variance $\\sigma^2$\n",
    "\n",
    "Let's see if statsmodels gives us the same results! For this we'll create a <strong>fake</strong> dataset:\n",
    "\n",
    "\n",
    "### Unbiased Autocovariance\n",
    "Note that the number of terms in the calculations above are decreasing.<br>Statsmodels can return an \"unbiased\" autocovariance where instead of dividing by $n$ we divide by $n-k$.\n",
    "\n",
    "$\\gamma_0 = \\frac {(13-10)(13-10)+(5-10)(5-10)+(11-10)(11-10)+(12-10)(12-10)+(9-10)(9-10)} {5-0} = \\frac {40} 5 = 8.0 \\\\\n",
    "\\gamma_1 = \\frac {(13-10)(5-10)+(5-10)(11-10)+(11-10)(12-10)+(12-10)(9-10)} {5-1} = \\frac {-20} 4 = -5.0 \\\\\n",
    "\\gamma_2 = \\frac {(13-10)(11-10)+(5-10)(12-10)+(11-10)(9-10)} {5-2} = \\frac {-8} 3 = -2.67 \\\\\n",
    "\\gamma_3 = \\frac {(13-10)(12-10)+(5-10)(9-10)} {5-3} = \\frac {11} 2 = 5.5 \\\\\n",
    "\\gamma_4 = \\frac {(13-10)(9-10)} {5-4} = \\frac {-3} 1 = -3.0$\n",
    "\n",
    "### Autocorrelation for 1D\n",
    "The correlation $\\rho$ (rho) between two variables $y_1,y_2$ is given as:\n",
    "\n",
    "### $\\rho = \\frac {\\operatorname E[(y_1−\\mu_1)(y_2−\\mu_2)]} {\\sigma_{1}\\sigma_{2}} = \\frac {\\operatorname {Cov} (y_1,y_2)} {\\sigma_{1}\\sigma_{2}}$,\n",
    "\n",
    "where $E$ is the expectation operator, $\\mu_{1},\\sigma_{1}$ and $\\mu_{2},\\sigma_{2}$ are the means and standard deviations of $y_1$ and $y_2$.\n",
    "\n",
    "When working with a single variable (i.e. <em>autocorrelation</em>) we would consider $y_1$ to be the original series and $y_2$ a lagged version of it. Note that with autocorrelation we work with $\\bar y$, that is, the full population mean, and <em>not</em> the means of the reduced set of lagged factors (see note below).\n",
    "\n",
    "Thus, the formula for $\\rho_k$ for a time series at lag $k$ is:\n",
    "\n",
    "${\\displaystyle \\rho_k = \\frac {\\sum\\limits_{t=1}^{n-k} (y_t - \\bar{y})(y_{t+k}-\\bar{y})} {\\sum\\limits_{t=1}^{n} (y_t - \\bar{y})^2}}$\n",
    "\n",
    "This can be written in terms of the covariance constant $\\gamma_k$ as:\n",
    "\n",
    "${\\displaystyle \\rho_k = \\frac {\\gamma_k n} {\\gamma_0 n} = \\frac {\\gamma_k} {\\sigma^2}}$\n",
    "\n",
    "For example,<br>\n",
    "$\\rho_4 = \\frac {\\gamma_4} {\\sigma^2} = \\frac{-0.6} {8} = -0.075$\n",
    "\n",
    "Note that ACF values are bound by -1 and 1. That is, ${\\displaystyle -1 \\leq \\rho_k \\leq 1}$\n",
    "\n",
    "### Partial Autocorrelation\n",
    "Partial autocorrelations measure the linear dependence of one variable after removing the effect of other variable(s) that affect both variables. That is, the partial autocorrelation at lag $k$ is the autocorrelation between $y_t$ and $y_{t+k}$ that is not accounted for by lags $1$ through $k−1$.\n",
    "\n",
    "A common method employs the non-recursive <a href='https://en.wikipedia.org/wiki/Autoregressive_model#Calculation_of_the_AR_parameters'>Yule-Walker Equations</a>:\n",
    "\n",
    "$\\phi_0 = 1\\\\\n",
    "\\phi_1 = \\rho_1 = -0.50\\\\\n",
    "\\phi_2 = \\frac {\\rho_2 - {\\rho_1}^2} {1-{\\rho_1}^2} = \\frac {(-0.20) - {(-0.50)}^2} {1-{(-0.50)}^2}= \\frac {-0.45} {0.75} = -0.60$\n",
    "\n",
    "As $k$ increases, we can solve for $\\phi_k$ using matrix algebra and the <a href='https://en.wikipedia.org/wiki/Levinson_recursion'>Levinson–Durbin recursion</a> algorithm which maps the sample autocorrelations $\\rho$ to a <a href='https://en.wikipedia.org/wiki/Toeplitz_matrix'>Toeplitz</a> diagonal-constant matrix. The full solution is beyond the scope of this course, but the setup is as follows:\n",
    "\n",
    "\n",
    "$\\displaystyle \\begin{pmatrix}\\rho_0&\\rho_1&\\cdots &\\rho_{k-1}\\\\\n",
    "\\rho_1&\\rho_0&\\cdots &\\rho_{k-2}\\\\\n",
    "\\vdots &\\vdots &\\ddots &\\vdots \\\\\n",
    "\\rho_{k-1}&\\rho_{k-2}&\\cdots &\\rho_0\\\\\n",
    "\\end{pmatrix}\\quad \\begin{pmatrix}\\phi_{k1}\\\\\\phi_{k2}\\\\\\vdots\\\\\\phi_{kk}\\end{pmatrix}\n",
    "\\mathbf = \\begin{pmatrix}\\rho_1\\\\\\rho_2\\\\\\vdots\\\\\\rho_k\\end{pmatrix}$\n",
    "\n",
    "\n",
    "### Auto Correlation function (ACF) and Partial Auto Correlation function (PACF)\n",
    "\n",
    "Here we learn about two very useful **plot** types. \n",
    "\n",
    " - ACF:  Auto Correlation Plot  \n",
    " - PCF\n",
    " \n",
    "To understand these terms we should learn what correlation means. It is a measure of the **strength of the linear relationship** between two variables. It can takes values between 1 and -1. The closer the value to positive (negative) 1 the stronger the positive (negative) linear relationship. As it gets closer to zero, the weaker relationship or association.\n",
    "\n",
    "**Note:** Correlation is not always refered to linear relationsip. But the common relationship is the linear one.\n",
    "\n",
    "**Auto Correlation Plot**\n",
    "\n",
    "An autocorrelation plot aka **Correlogram** shows the correlation of the series with itself, lagged by x time units. Hence, the y-axis is the correlation and x-axis is the number of time units of lag. Ex: How today's sale is correlated to the yesterday's data? Note that yesterday is shifted by one relative to today. Then plot of correlation for other days in one plot gives the autocorrelation plot.\n",
    "\n",
    "\n",
    "Types of Auto-Correlation:\n",
    "\n",
    "   - **Gradual Decline:**\n",
    "    As we shift data more and more, we are less likely to    be correlated with the data that happend long time ago. \n",
    "\n",
    "   - **Sharp Dorp-Off:**\n",
    "When sharp drop off occurs after one or two units of shift.\n",
    "\n",
    "Generally decreasing in auto-correlation makes sense, since as time passes, it's less likely that data is correlated with itself. \n",
    "\n",
    "**Partial Auto correlation plot**\n",
    "\n",
    "It's again is a relationship between a time series and itself. It shows the direct relationship between an observation and its lag. \n",
    "\n",
    "ACF and PACF are used to find the parameters of ARIMA \n",
    "based models.\n",
    "\n",
    "\n",
    "**Important Note**\n",
    "\n",
    "In the case of non-stationary time series, the acf plots show a gradual decline and in the case of stationary timeseries show a sharp drop-off.\n",
    "\n",
    "ACF and PACF **Codes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.tsa.stattools import acovf,pacf, acf, pacf_yw, pacf_ols \n",
    "\n",
    "acovf(df['col_name'])\n",
    "acf(df['col_name'])\n",
    "pacf(df['col_name'])\n",
    "pacf_yw(df['col_name'])\n",
    "pacf_ols(df['col_name'])\n",
    "\n",
    "# Import library for plots\n",
    "from pandas.plotting import lag_plot\n",
    "\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "\n",
    "plot_acf(df_name,lags=)\n",
    "\n",
    "# in the above plot the shaded region shows a 95% confidence interval\n",
    "# which suggests that correlation values outside of this confidence interval\n",
    "#are very highly likely to be correlated.\n",
    "\n",
    "plot_pacf(df_name,lags=)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ARIMA Overview\n",
    "\n",
    "**ARIMA** is one of the most common time series models. Many models are based off the ARIMA model, which stands for AutoRegressive Integrated Moving Average. It has three components 1. AR 2. I 3. MA. Note that ARIMA is not capable of perfectly predicting any time series data. Beginners often want to directly apply ARIMA to time series data that is not directly a function of time, such as stock data. Many features play role in predicting the price of a stock. So, only knowing the price of stock over time is not enough to predict the price of a stock. ARIMA performs really well when we are working with a time series where the **data is directly related to the time stamp**, such as the airline passenger data set. However, by using ARIMA model we wolud not be able to predict the effect of outside factors.\n",
    "\n",
    "**Important Note:** ARIMA based models are extremely powerful but they are not magic! Large part of using them effectively is **understanding our data**.\n",
    "\n",
    "**ARIMA** model basically is a generalization of **ARMA** model which stands for Auto Regression Moving Average. These models can be fitted to time series data either to better **understand** data or to **predict** future points(forecasting). \n",
    "\n",
    "**Types of ARIMA model**\n",
    "\n",
    "    1. Non-seasonal ARIMA model\n",
    "    2. Seasonal ARIMA model (SARIMA)\n",
    "    \n",
    "ARIMA models are applied in some cases where data show evidence of non-stationarity, where an initial differencing step(corresponding to the 'integrated' part of the model) can be applied one or more times to eliminate non-stationarity.      \n",
    "\n",
    "\n",
    "**Non-seasonal ARIMA model**\n",
    "\n",
    "Non-seasonal ARIMA models are generally denoted **ARIMA(p,d,q)** where parameters p,d, and q are non-negative integers. \n",
    "\n",
    "**Parts of ARIMA model**\n",
    "\n",
    "    1. AR(p): Autoregression: A regression model that utilizes the dependent relationship between a current observation and observations over a period.\n",
    "    2. I(d): Integrated: Differencing of observations (Substracting an observation from an observation at the previous time step) in order to make the time series **stationary**.\n",
    "    3. MA(q): Moving Average: A model that uses the dependency between an observation and a residual error from a moving average model applied to lag observation.  \n",
    "    \n",
    "    \n",
    "To effectively use ARIMA, we need to understand Stationarity in our data.\n",
    "\n",
    "What a makes a data set **stationary**?\n",
    "\n",
    "A staionary series has **constant mean and variance** over time. \n",
    "\n",
    "A stationary data set will allow our model to predict that the mean and variance will be the **same** in the future periods. \n",
    "\n",
    "for **constant mean** over time there should be no trend in the data set. Note that **covariance** also should not be a function of time. \n",
    "\n",
    "A common **mathematical test** that can be used to check the stationarity is **Augmented Dickey-Fuller** test.\n",
    "\n",
    "If we've determined that the data set is **not** stationary we should transform it to be **stationary** in order to evaluate it and what type of ARIMA terms we will use. \n",
    "\n",
    "We use **differencing** to make the data stationary. Differencing has order first, second and so on. We can continue differencing until we reach something stationary. Each diffrencing comes at the cost of losing a row of data.\n",
    "\n",
    "For seasonal data, we can also difference by season. For example, if we has a monthly data with yearly seasonality, we can difference by a time unit of 12, instead of just 1. \n",
    "\n",
    "Another common technique with seasonal ARIMA models is to combine both methods, taking the seasonal difference of the first difference. \n",
    "\n",
    "Lets now consider that the data is **stationary** and discuss p,q, and d and how to choose them. There are two main ways to choose p,d, and q terms. \n",
    "\n",
    "    1. Method One (more difficult): Using ACP and PACP and view the decay in the plot. However, these plots can be very difficult to read and often even when reading them correctly, the best performing p,d, or q value may be different than what is read. \n",
    "    2. Method Two (Easy but takes time): Grid Search. In this method we run bunch of ARIMA based models on different combinations of p,d, and q and compare the models using evaluation metrics\n",
    "\n",
    "\n",
    "### AutoRegression (AR):\n",
    "\n",
    "This model is one of the components of the ARIMA model where the integrated and moving average have been dropped.\n",
    "\n",
    "In an autoregression model, we forecast using a linear combiation of past values of a variable. The term autoregression describes a regression of the variable against itself. An autoregression is run against a set of lagged values of order **p**. \n",
    "\n",
    "What is the purpose fo the **Autoregression** model?\n",
    "\n",
    "The **AutoRegressive** model specifies that the output variable depends **linearly** on its own previous values and on stochastic term (an imperfectly predictable term). \n",
    "\n",
    "In an autoregression model, we forecast using a linear combination of <em>past values</em> of the variable. The term <em>autoregression</em> describes a regression of the variable against itself. An autoregression is run against a set of <em>lagged values</em> of order $p$.\n",
    "\n",
    "### $y_{t} = c + \\phi_{1}y_{t-1} + \\phi_{2}y_{t-2} + \\dots + \\phi_{p}y_{t-p} + \\varepsilon_{t}$\n",
    "\n",
    "where $c$ is a constant, $\\phi_{1}$ and $\\phi_{2}$ are lag coefficients up to order $p$, and $\\varepsilon_{t}$ is white noise.\n",
    "\n",
    "For example, an <strong>AR(1)</strong> model would follow the formula\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;$y_{t} = c + \\phi_{1}y_{t-1} + \\varepsilon_{t}$\n",
    "\n",
    "whereas an <strong>AR(2)</strong> model would follow the formula\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;$y_{t} = c + \\phi_{1}y_{t-1} + \\phi_{2}y_{t-2} + \\varepsilon_{t}$\n",
    "\n",
    "and so on.\n",
    "\n",
    "Note that the lag coeffients are usually less than one, as we usually restrict autoregressive models to stationary data.<br>\n",
    "Specifically, for an <strong>AR(1)</strong> model: $-1 \\lt \\phi_1 \\lt 1$<br>\n",
    "and for an <strong>AR(2)</strong> model: $-1 \\lt \\phi_2 \\lt 1, \\ \\phi_1 + \\phi_2 \\lt 1, \\ \\phi_2 - \\phi_1 \\lt 1$<br>\n",
    "\n",
    "Models <strong>AR(3)</strong> and higher become mathematically very complex. Fortunately statsmodels does all the heavy lifting for us.\n",
    "\n",
    "AR model **codes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import library\n",
    "\n",
    "from statsmodels.tsa.ar_model import AR, ARResult"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the data into train/test sets\n",
    "\n",
    "Befor forecasting we must split the data set into train and test sets.\n",
    "\n",
    "The goal in this section is to:\n",
    "* Split known data into a training set of records on which to fit the model\n",
    "* Use the remaining records for testing, to evaluate the model\n",
    "* Fit the model again on the <em>full</em> set of records\n",
    "* Predict a future set of values using the model\n",
    "\n",
    "As a general rule you should set the length of your test set equal to your intended forecast size. That is, for a monthly dataset you might want to forecast out one more year. Therefore your test set should be one year long."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To split the time series\n",
    "\n",
    "len(df)\n",
    ">>96\n",
    "train = df.iloc[:84]\n",
    "test = df.iloc[84:]\n",
    "\n",
    "\n",
    "\n",
    "# Fit different orders of AR(p)\n",
    "\n",
    "model = AR(train['col_name'])\n",
    "AR1fit = model.fit(maxlag=1,method='mle')\n",
    "\n",
    "# Here maxlag specifies the order of AR. maxlag=p\n",
    "# If we leave the maxlag as none \n",
    "# statsmodels finds the optimized value for p\n",
    "#ex:\n",
    "\n",
    "ARfit = model.fit(method='mle')\n",
    "\n",
    "ARfit.k_ar   # gives the order of AR model\n",
    "ARfit.params  # gives the parameters of Auto regression\n",
    "\n",
    "### PREDICTION\n",
    "\n",
    "# for making predictions we need to find the start and end points \n",
    "\n",
    "start = len(train)\n",
    "end = len(train)+len(test)-1\n",
    "\n",
    "\n",
    "predictions1 = AR1fit.predict(start=start, end=end)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the Model\n",
    "We can use mean squared error to evaluate the model. And also we can use **Akaike information criterion (aic)** to evaluate the performance of the model. The **aic** method is available also in the fit model object. \n",
    "\n",
    "Example is wirtten below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AR(df['col_name'])\n",
    "ARfit = model.fit(method='mle')\n",
    "\n",
    "ARfit.aic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forecasting\n",
    "\n",
    "Now to forecast future data we use the piece of code written in the below box."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, retrain the model on the full dataset\n",
    "model = AR(df['col_name'])\n",
    "\n",
    "# Next, fit the model\n",
    "ARfit = model.fit(maxlag=11,method='mle')\n",
    "\n",
    "# Make predictions\n",
    "fcast = ARfit.predict(start=len(df), end=len(df)+12, dynamic=False).rename('Forecast')\n",
    "\n",
    "# Plot the results\n",
    "df['col_name'].plot(legend=True)\n",
    "fcast.plot(legend=True,figsize=(12,6));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descriptive statistics\n",
    "\n",
    "The purpose of this section is to mathematically check to see if a data series **Stationary** or not. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep learning Models\n",
    "\n",
    "### Recurrent Neural Network\n",
    "\n",
    "Recurrent Neural networks (RNN) are specifically designed for working with **sequence** data types such as:\n",
    "\n",
    "    - Time Series Data\n",
    "    - Sentences\n",
    "    - Audio\n",
    "    - Car Trajectories\n",
    "    - Music\n",
    "    \n",
    "We can consider a **sequence** as just a vector of information where the index location are the points in time and the values are the variable we want to predict. \n",
    "\n",
    "**How RNN works**\n",
    "\n",
    "In a ordinary neural network each neuron takes an input or set of inputs and aggregate them the pass it through some sort of activation function (Rectified Linear unit, sigmoid function, tanh, an so on) and from that we have output. The process in a **Recurrent Neuron** is a little different. It sends output back to itself. The recurrent network is then made of unrolling the recurrent neurons during time. \n",
    "\n",
    "**Unrolling process**\n",
    "\n",
    "The first recurrent nuron takes an input of time t, $X_t$. Then outputs a value $y_t$ which will be used as the input of the same neuron and also its adjacent neuron in time (Note that the adjacent neuron in time is at t+1). Then the input for the neuron at t+1 is the output of the former neuron $y_t$ and also the input at t+1 which is $X_{t+1}$. This process is called unrolling and repeats until we forecast a value. \n",
    "\n",
    "### Long Short Term Memory (LSTM) and GRU\n",
    "\n",
    "An issue that RNN faces is that after a while the network will begin to forget the first inputs. As information is lost at each step going through The RNN, we need some sort of **long-term memory** for our networks. The LSTM cell was created to help address these RNN issues. \n",
    "\n",
    "**How LSTM works?**\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
