{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Notes\n",
    "\n",
    "## 1. Introduction\n",
    "\n",
    "**Machine learning** is a field of study that gives the computers the ability to learn from without being explicitly programmes(Arthur Samuel). \n",
    "\n",
    "-**Training Set** includes the examples that the system uses to learn and each example is called to be **training instance or sample. \n",
    "\n",
    "-**feature or attribute** The part of the training set that helps the system to predict\n",
    "\n",
    "-**label or target** The part of the data set must be predicted by system. When we have them in the training set, we call the model **supervised**\n",
    "\n",
    "-**Classification problem** When the label or target is discrete. We expect the model to predict the class of the label.\n",
    "\n",
    "-**Regression problem** When the label or target is a continuous number(price of a car or home). We expect the model to predict a number. \n",
    "\n",
    "-**Linear Regression** In this model we are trying to fit a linear line to the data and use the parameters of that linear data to make predictions.\n",
    "\n",
    "-**Cost function, Utility function or fitness function** Cost function measures how bad the model is. Utility or fitness function measures how good the model is. \n",
    "\n",
    "-**Note** Some regression algorithms can be used for classification problems as well and vice vers. For instance, logistic regression that predicts the probability of belonging to a given class.(20% chance of being spam)\n",
    "\n",
    "-**Accuracy** In order to measure the performance of machine after learning we need to define performance measurement. The ratio of correctly classified examples is called to be accuracy.\n",
    "\n",
    "-An advantage of machine learning algorithms over the traditional programing is that it doesn't require intervention since it learns from data\n",
    "\n",
    "-**data mining** When we use machine learning techniques to dig into large amounts of data to discover patterns that are not immediately apparent.\n",
    "\n",
    "-**Clustering Algorithm** The Algorithms that detects the group of similar data(unsupervised).\n",
    "\n",
    "-**Hierarchical Clustering Algorithms** Enables us to subdivide each group into smaller groups (Unsupervised).\n",
    "\n",
    "-**Dimensionality Reduction** In this algorithm the goal is to simplify data without loosing too much information. One way to do that is to merge several correlated features into one. It's often a good idea to use dimensionality reduction algorithms to reduce dimension of the training set. It increses the calculation speed and in some cases may also perform better.\n",
    "***\n",
    "\n",
    "-**Types of Machine Learning systems** \n",
    "- **Supervised, unsupervised, semi-supervised, reinforcement.** This type depends on the relation between the system and human supervision. \n",
    "- **Online, batch learning.** This type depends if the system can learn incrementally on the fly\n",
    "- **Instance based, model based.** This type depends if the system compares the new data points to known data or the system finds a pattern in the training set and builds a predictive model.\n",
    "***\n",
    "-**Some of the most important supervised learning algorithms**\n",
    "\n",
    "**Supervised learning** In this model the training set we feed to the algorithm includes the desired solutions called **labels**. \n",
    "\n",
    "- k-Nearest Neighbors\n",
    "- Linear Regression\n",
    "- Logistic Regression\n",
    "- Support Vector Machine (SVM)\n",
    "- Decision Trees and Random Forests\n",
    "- Neural Networks\n",
    "\n",
    "**Some of the most important unsupervised learning algorithms**\n",
    "\n",
    "**Unsupervised learning** As it can be guessed the training data set is unlabeled. So the system tries to learn without teacher.\n",
    "\n",
    "- Clustering:\n",
    "  \n",
    "  1. K-means\n",
    "  \n",
    "  2. DBSCAN\n",
    "  \n",
    "  3. Hierarchical Cluster Analysis(HCA)\n",
    "\n",
    "- Anomaly detection and novelty detection:\n",
    "\n",
    "Finds the outliar or novel data points of a data set. It's useful to apply it before feeding data to other algorithms\n",
    "\n",
    "   1. One class SVM\n",
    "   2. Isolation Forest\n",
    "\n",
    "- Visulaization and dimensionality reduction\n",
    "\n",
    "- Association Rule:\n",
    "\n",
    "Useful when we dig into large amount of data and discover interesting relations between attributes (features). Example: Suppose we run a supermarket. Running an association rule on the data set that includes sales logs, reveal that people who purchase barbecue suace and potato chips also tend to buy steak. So we place them close each other.\n",
    "\n",
    "\n",
    "**Semi-supervised Learning** The Algorithms that deals with data sets that are partially labeled. Note that labeling is usually time consuming and costly. \n",
    "\n",
    "\n",
    "**Reinforcement Learning** In this model, the learning system called **Agent** can observe the environment, select and perform actions and get rewards or penalties in return. It must then learn itself what is the best strategy to get the most reward over time. The actions or strategies chosen by agent is called to be **policy**.\n",
    "\n",
    "***\n",
    "\n",
    "**Batch learning or Offline learning** When the system is incapable of learning incrementally that means it must be trained using all available data. As it is obvious, it will generally take a lot of time and computing resources. In this learning type, first the system is trained. Then it runs without learning. \n",
    "\n",
    "**Online or incremental learning** When the system learns incrementally by feeding it data instances sequentially, either individually or in small groups called mini-batches. \n",
    "***\n",
    "\n",
    "**Instance based learning** The system learns the examples and generalize it to new cases by using *measure of similarity* to compare the learned example and the new case.\n",
    "\n",
    "**Model based learning** Another way of learning is making a model of the examples and then use that model to make predictions.\n",
    "***\n",
    "**Main Challenges of machine learning** The main task of machine learning is choosing an algorithm and train it on some data. So, two thing can go wrong, first *bad algorithm* and second *bad daata*\n",
    "\n",
    "**Examples of bad data**\n",
    "\n",
    "  1. Insufficient quantity of training data: data and algorithm have a mutual relationship. The lack of training data  can prevent the algorithm from making good predictions while enough training data can help different algorithms       including the simple ones to make almost same predictions.\n",
    "\n",
    "  2. Nonrepresentative Training Data: In order to generalize (generalize means making prediction for new examples) a model the training data should be the representative of new cases that we want to generalize to. That's true for both instance and model based learning. Very small samples are nonrepresentative since we have *sampling noise*. Even very large samples can be nonrepresentative if the sampling method is flawed which is called *sampling bias*.\n",
    "  \n",
    "  3. Poor-quality data: We should take care of outliars or missing data\n",
    "  \n",
    "  4. Irrelevant Features: A machine learning algorithm will be successful if it comes up with a good set of features. The process of preparing good features is called *feature engineering* which involves two steps:\n",
    "    - feature selection(Selecting the most useful features to work train on among existing features.)\n",
    "    - feature extraction: (Combining existing featuresto produce more useful one)\n",
    "    - creating new features by gathering new data\n",
    "\n",
    "**Examples of bad algorithm**\n",
    "\n",
    "  1. Overfitting the training data: When the model performs well on the training data, but it does not generalize well. Overfitting happens when the model is too comples relative to the amount and noisiness of the training data. Some possible solutions are\n",
    "    - Simplifiying the model by selecting the one with fewer parameters. Constraining a model to make it simpler and reduce risk if overfitting is call **regularization**. The amount of *regularization* to apply during learning can be controlled by a **hyperparameter**. A *hyperparameter* is a parameter of the learning algorithm not the model. So, it's not affected by the learning algorithm itself and it must be set prior to training and remains constant during training.\n",
    "    - Gather more training data\n",
    "    - Reduce noise in training data\n",
    "     \n",
    "  2. Unserfitting the training data: Underfitting is the opposite of the overfitting. It occurs when the model is too simple to learn the underlying structure of the data. The following are the main options for fixing the underfitting problem\n",
    "  \n",
    "   - Select a more powerful model with more parameters\n",
    "   - Feed better features to the learning algorithm (better feature enginnering)\n",
    "   - Reduce the constraints on the model (Reduce the regularization hyperparameter)   \n",
    "***   \n",
    "  \n",
    "**Testing and Validating**\n",
    "\n",
    "One way to test how well the model will perform, is to generalize it to new cases and monitor it to see how well it performs. However, this method is not practical. A better option is to split the data set into two sets: the **training set** and **test set**. We train the model using the train set then test it by the test set. Then, the error rate on new examples are called **generalization error (out of sample error)**. Test set helps to evaluate an estimation of this error before generalization. If the training error is low but the generalization error is high, the model is overfitting the training data. \n",
    "\n",
    "**Note**: It is common to use 80% of data for training and hold out 20% for training. However, it also depends on the size of data set as well. \n",
    "\n",
    "**Hyperparameter Tuning and Model Selection**\n",
    "\n",
    "One way to deal with overfitting is regularization (reducing the degrees of freedom). Suppose we have chosen a regularization hyperparameter that works well for the training set, while its performance is poor in generalization. This happens because we have tuned the hyperparameter for the test set that does not assure its good performance on new data. To resolve this issue we hold out some part of training set to evaluate several candidate models and select the best model. This set is called **validation set (development set or dev set)**. Here is the scenario: We train different models on the trainind set (without validation set) with different hyperparameters and choose the model that performs best on the validation set. Then, we train the chosen model on the full training set before its evaluation on the test set. To optimized this method, we choose many small validation sets. Each model is evaluated once per validation set after being trained on the rest of the data. By averaging over the evaluations of a model, we get much more accurate measure of its performance. This method is called **cross validation**. One draw back of cross validation is that the training time is multiplied by the number of validation sets.  \n",
    "\n",
    "**Data mishmatch** \n",
    "\n",
    "page 32. \n",
    "\n",
    "**No Free Lunch Theorem** \n",
    "\n",
    "Usually we choose the model that we want to train based on the assumptions that we make on the data sets (for instance we discard superfluous details). If we make absolutely no assumptions about data, then there is no reason to prefer one model over other -**No free lunch (NFL) Theorem**-. So there is no model priori guaranteed to work better. We can train all models on the data and evaluate them to test which one performs better, but it's impossible.  \n",
    "\n",
    "***\n",
    "## 2. End to end Machine learning project\n",
    "\n",
    "**Grid Search**\n",
    "\n",
    "When we have a list of promising models, we need a way to fine tune the model. GridSearch is one of these ways. One way is to just adjust the hyperparameters manually, until we find the best comnination. Since, it's a tedious process, we should get Scikit Learn's **GridSearchCV** to search for us. All we need to do is specify the hyperparameters we want to experiment with and what values to try out. Then, it uses **Cross Validation** to evaluate all the possible combinations of hyperparameter values.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GridSearch Cross validation code for RandomForestRegressor\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = [{'n_estimators':[3,10,30], 'max_features': [2, 4, 6, 8]},\n",
    "             {'bootstrap': [False], 'n_estimators':[3,10], 'max_features':[2,3,4]}]  #The parameter grid includes\n",
    "# all the hyperparameters that we want to be tested. The GridSearch first evaluate 3x4=12 combinations of n_estimators\n",
    "# and max_features hyperparameter in the first dict and then evaluates 2x3=6 combinations this time with bootstrap False.\n",
    "#The total combination is 12+6=18 and it will train each model 5 times (cv=5), so we have 18*5=90 rounds of training. \n",
    "\n",
    "forest_reg = RandomForestRegressor()\n",
    "\n",
    "grid_search = GridSearchCV(forest_reg, param_grid, cv=5, scoring='neg_mean_squared_error', return_train_score=True)\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "grid_search.best_params_  #gives the best combination\n",
    "\n",
    "grid_search.best_estimator_ # also gives all the information of the best model\n",
    "\n",
    "# if we initialize the GridSearchCV with refit=True, then once ot finds the best estimator using cross validation,\n",
    "# it retains it on the whole training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## 3. Classification\n",
    "\n",
    "**Cross Validation**\n",
    "\n",
    "A good way to evaluate a classifier is to use **cross validation**. The cross validation measures the number fo accuaret predictions (accuracy). Hence, it's a good method for evaluation of balanced datasets but not **skewed datasets**(when some classes are much more frequent than others).\n",
    "\n",
    "**Codes**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This uses 3-fold-cross-validation\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "cross_val_score(model_name, X_train, y_train, cv=3, scoring='accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Confusion Matrix**\n",
    "\n",
    "**Precision and Recall**\n",
    "\n",
    "**f1-score**\n",
    "***\n",
    "## 4. Training Models (Linear models)\n",
    "**Linear Regression, Logistic Regression, Polynomial Regression**\n",
    "\n",
    "**Udemy Course:**\n",
    "\n",
    "**Linear Regression** is a model that fits a linear line to a set of data. One procedure to choose the best line that fits the data **least squares method**.\n",
    "\n",
    "**Least Squares Method**: In this method we minimize the sum of sqaures of the residuals. The residuals for an observation is the difference between the observation (the known value) and the fitted line.\n",
    "\n",
    "**Book**\n",
    "\n",
    "In this chapter we learn linear regression discussing two very different ways to train it:\n",
    "\n",
    "  . Closed form equation that directly computes the model parameters that best fit the model to the training set(It chooses the model parameters that minimizes the cost function)\n",
    "  \n",
    "  . Using an iterative method called **gradient descent** that gradually tweaks the model parameters to minimize the cost function\n",
    "  \n",
    "**Polynomial Regression** is a model that fits to non-linear data\n",
    "\n",
    "\n",
    "**Linear Model** basically makes a prediction using a wieghted sum of some input features plus a bias term. \n",
    "\n",
    "**Linear Regression Eq.**\n",
    "\n",
    "$\\hat{y} = \\theta_0+\\theta_1X_1+\\theta_2X_2+.....+\\theta_nX_n$\n",
    "\n",
    ", where $\\hat{y}$ is the predicted value, $\\theta_0$ is the bias term, $\\theta_1 ... \\theta_n$ are the weights, and $X_1 ... X_2$ are the features. \n",
    "\n",
    "Linear regression in **vectorized** format:\n",
    "\n",
    "$\\hat{y}=h_{\\theta}(X)=\\bf{\\theta}.\\bf{X}$\n",
    ", where $X_0$ is always equal to one.\n",
    "\n",
    "To train the model we find the $\\bf{\\theta}$ so that it minizes the root mean squared error (RMSE), since it's the most common performance measure of a regression model. I must be pointed out that it is simpler to minimize the mean squared error (MSE) practically. Both leads to same results since, if a value that minimizes a function will minimize its square root as well.\n",
    "\n",
    "\n",
    "MSE($\\bf{X},h_{\\bf{\\theta}}$) = $\\frac{1}{m}\\sum_{i=1}^m (\\theta^T\\bf{X}^{(i)}-y^{(i)})^2$\n",
    "***\n",
    "**Normal Equation**\n",
    "\n",
    "The mathematical equation that gives the set of parameters that minimizes the above equation can be obtained directly by using **Normal Equation**\n",
    "\n",
    "$\\hat{\\theta}=(X^TX)^{-1}X^Ty$\n",
    "\n",
    "The Scikit-Learn commands for linear regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lin_reg=LinearRegression()\n",
    "lin_reg.fit(X,y)\n",
    "lin_reg.intercept_, lin_reg.coef_\n",
    "lin_reg.predict(X_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The LinearRegression() uses **scipy.linalg.lstsq()** function (least square function) to calculate this equation. This function computes pseudoinverse of $\\bf{X}$ and then computes $\\theta$. To calculate the pseudoinverse we can use the **singular value decomposition (SVD)**.\n",
    "\n",
    "**Note** The method of pseudoinverse is more advantagous due to the fact that the noraml equation is not defined when the ($X^TX$) is not invertible.But, pseudoinverse matrix is always defined.\n",
    "\n",
    "***\n",
    "**Computational Complexity**\n",
    "\n",
    "Both the Normal equation and SVD approach get very slow when the number of **features** grow large (Note that the number of features is different from the number of instances). The computational complexity of the Normal Equation is about O($n^3$) and it's O($n^2$) for SVD approach. This means that doubling the number of features would multiply the computational time of Normal Equation by 8 and multiply by 4 in the case of SVD.(The typical number of features that we can use these two methods is 100,000). The positive side is that the computational complexity with respect to the number of instances is linear for both approaches. Also, once we have trained the LinearRegression() model the prediction is fast. The computational complexity of the prediction for both the number of instances and the number of features is linear.\n",
    "\n",
    "We use **Gradient Descent** method to train a Linear Regression model for the problems with large number of features.\n",
    "***\n",
    "\n",
    "**Gradient Descent (Optimization algorithm)**\n",
    "\n",
    "is a generic **optimization** algorithm capable of finding optimal solutions of many problems. The general idea of gradient descent is to tweak parameters iteratively to minimize cost function\n",
    "\n",
    "**What does the gradient descent?**\n",
    "\n",
    "It measures the **local gradient** of the error function (cost function) with respect to the parameter vector $\\theta$. Then, it goes in the direction of the descending gradient. Once the gradient is zero, we have reached the minimum of the cost function. We start with a random vector parameter called **random initialization**, then we take small steps from the random initializaion value to converge to a minimum. The important step of gradient descent is the **size of steps**, determined by **learning rate** hyperparametr. If the steps are too small, then we'll have to go through many iterations to reach minimum. If the steps are too large we might skip the minimum and convergence. One of the drawbacks of the gradient descent algorithm is the presence of the local minima in the system. However, MSE cost function for a Linear Regression model happens to be a **convex function**. This means that if we pick any two points on the curve, the line segment joining them never crosses the curve. This means that MSE cost function for a regression model has **no** local minima and just one **global minimum**. Furthermore, it is a continuous function with a smooth slope that does not change abruptly. These two properties guarntee that if we wait for enough and take small steps we obtain the minimum. \n",
    "\n",
    "**IMPORTANT NOTE** When we use gradient descent we must ensure that all the features have a similiar scale. For this purpose we should use the **StandardScalar class** from scikit-learn. \n",
    "\n",
    "*** \n",
    "**Three ways of implementing gradient descent**\n",
    "\n",
    "**1. Batch Gradient Descent**\n",
    "\n",
    "To implement gradient descent we need to compute the gradient of the cost function with respect to each model parameter. The gradient formula can be written as\n",
    "\n",
    "$\\nabla$MSE($\\theta$)=$\\frac{2}{m}X^T(X\\theta-y)$\n",
    "\n",
    "In the batch gradient descent method for each step we use the whole training set. That makes this algorithm slow. However the gradient descent scales well with the number of features since its just multiplication of the vectors.\n",
    "\n",
    "**How to determine the gradient descent steps?**\n",
    "\n",
    "We calculate the gradient at each value of the parameter vector $\\theta$, then if it is increasing we should change direction by substracting the $\\nabla$MSE($\\theta$)$ multiplied by learning rate $\\eta$ from $\\theta$. Here is the formula,\n",
    "\n",
    "$\\theta^{(next step)}=\\theta-\\eta\\nabla_{\\theta}MSE(\\theta)$\n",
    "\n",
    "**Note:** In the batch gradient descent method the learning rate $\\eta$ and the number of iterations (number of steps that we take) are really important. For low learning rate it takes long time to converge to the optimal solution. For **high** learning rate the algorith skips the optimal solution and **diverges**. To find the best learning rate we can do a **grid search**. Since it's not known what the number of steps should be at the first step, we can choose a very large number but interupt the algorithm if the gradient vector becomes a tiny number $\\epsilon$ (tolerance). Since the gradient becomes very small (zero exactly) at the minimum.\n",
    "\n",
    "**2. Stochastic Gradient Descent**\n",
    " \n",
    "The problem with batch gradient descent is the fact that it uses the whole training set to compute the gradient at each step. So, the size of the training set is large this algorithm is slow. On the other hand, **stochastic gradient descent** resolves this problem by choosing only one random instance from the training set. Unlike the batch gradient descent, the stochastic gradient descent does not decrease gradually to reach the minimum. Instead the gradient bounces up and down which decreases only on average. Over time it will end up very close to minimum but, but when it gets there it will continue to bounce around, never settling down. Hence, when the algorithm stops, the parameters are good but **not optimal**.\n",
    "\n",
    "**Note:** Due to it's nature, the stochastic GD has a better chance of skipping local minima and finding global minima than batch GD. So, randomness of stochastic GD is good to escape from local minima, bu bad because it means that the algoritm never settle at the minimum. \n",
    "\n",
    "**How to solve the dilemma of never settling down?**\n",
    "\n",
    "Onse solution is that to **reduce the learning rate gradually**. To this end, we use  **simulated annealing** process. At the begining we start with a large learning rate (it skips the local minima), then it gets smaller and smaller, allowing the algorithm reaches minimum. The function that determines the learning rate at each step is called to be **learning schedule**. It should be noted that if the learning rate is reduced too quickly, we may get stuck in local minimum. \n",
    "\n",
    "**Very very important Note:** When we use the stochastic gradient descent method the training instances must be independent and identically distributed(IID) to ensure that the parameters get pulled toward the global optimum, on average. \n",
    "\n",
    "The Scikit-learn code of stochastic gradient descent (**SGDRegressor()** which defaults to optimizing the squared error cost function) is written below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDRegressor  # import SGD regression class\n",
    "\n",
    "sgd_reg = SGDRegressor(max_iter=1000,tol=1e-3,penalty=None,eta=0.1)\n",
    "sgd_reg.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Mini Batch Gradient Descent**\n",
    "\n",
    "This method is a combination of batch GD and stochastic GD. At each step, instead of computting the gradients based on the full training set,or based on just one instance, **mini-batch** computes gradients on small random sets of instances called mini-batches. \n",
    "***\n",
    "### The Bias/Variance Trade-off:\n",
    "\n",
    "We have three very different generalization (when the model predicts new values it's called generalization) error. \n",
    "\n",
    "  1. bias: The part of generalization error is due to wrong assumptions such as assuming that the data is linear while it's quadratic\n",
    "  \n",
    "  2. variance: This part of generalization error is due to excessive sensitivity to small variations in the training data set.\n",
    "  \n",
    "  3. Ireducible error: It's due to noisiness of the data itself. To reduce this error we should clean up data.\n",
    "  \n",
    "Increasing a **model's complexity** will typically increase its **variance** and reduce its **bias**. Conversely, reducing a model's complexity will reduce its variance and increase its bias. That's why it's called a **trade-off**.   \n",
    "\n",
    "***\n",
    "\n",
    "### Logistic Regression:\n",
    "\n",
    "Logistic regression is a regression algorithm used for **classification**. It is used to estimate the probability that an instance belongs to a particular class. If the probability is greater than a threshold, the model predicts that this instance belongs to that class. \n",
    "\n",
    "**Estimating probabilities**\n",
    "\n",
    "How does Logistic regression work? Just like a linear regression model, it computes the the weighted sum of the input features, but instead of outputing the result directly like the linear regression model does, it outputs the logistic of this result. The estimated probability of logistic regression model is\n",
    "\n",
    "$\\hat{p} = h_{\\bf{\\theta}}(X)=\\sigma(X^T\\theta)$.\n",
    "\n",
    "The logistic denoted by $\\sigma()$ is a **sigmoid function** that outputs a number between 0 and 1. The logistic function has the form\n",
    "\n",
    "$\\sigma(t)=\\frac{1}{1+e^{-t}}$. \n",
    "\n",
    "Once the logistic regression calculates the **probability** that an instance X belongs to the positive class, it can make its prediction $\\hat{y}$ easily. If the probability is greater than or equal to 0.5, it belongs to class positive class or class 1 and when the probability is less than 0.5, it belongs to class negative or class 0. Since $\\sigma(t)<0.5$ when $t<0$, and $\\sigma(t)>=0.5$ when $t>=0$, so a logistic regression model predicts 1 if ($X^T\\theta$) is positive and 0 if it is negative.\n",
    "\n",
    "**Training and Cost Function of Logistic Regression**\n",
    "\n",
    "The cost function that should be minimizes to compute the value of parameter vector is called **log loss** and the the form\n",
    "\n",
    "$J(\\theta)=-\\frac{1}{m}\\sum_{i=1}^{m}[y^{(i)}log(p^{(i)})+(1-y^{(i)})log(1-p^{(i)})]$.\n",
    "\n",
    "The bad news is that there is no known closed from equation to minimize the log loss function (an equation equivalent to normal equation in linear regression model). The good news is that this cost function is convex, so Gradient Descent or any other optimization algorithm is gauranteed to find the golbal minimum. The logistic cost function partial derivatives has the following form\n",
    "\n",
    "$\\frac{\\partial}{\\partial\\theta_j}J(\\theta)=\\frac{1}{m}\\sum_{i=1}^m(\\sigma(\\theta X(i))-y^{(i)})X_j^{(i)}$\n",
    "\n",
    "**Logistic Regression Code**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "log_reg = LogisticRegression()\n",
    "log_reg.fit(X,y)\n",
    "\n",
    "log_reg.predict_probab(X_new) # Predicts the probability of belonging to each class\n",
    "log_reg.predict(X_new)        # Predicts the class based on the calculated probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax Regression\n",
    "\n",
    "It's the generalization of the logistic regression to support multiple classes. It is also called **multinomial Logistic Regression**. \n",
    "\n",
    "**Estimate Probabilies**\n",
    "\n",
    "In this model for a given instance, we first calculate a score $s_k(\\bf{X})$ for each class k, then estimates the probability of each class using softmax function. The calculated score of class k can be written as \n",
    "\n",
    "$s_k(\\bf{X})=X^T\\theta^{k}$.\n",
    "\n",
    "Note that each class has its own dedicated parameter vector $\\theta^{(k)}$. Then, all the vectors are stored as rows in a parameter matrix $\\Theta$.\n",
    "\n",
    "The **softmax function** that computes the probabilities can be written as\n",
    "\n",
    "$p_k = \\sigma(s(X))_k = \\frac{exp(s_k(X))}{\\sum_{j=1}^K exp(s_j(X))}$.\n",
    "\n",
    "Then the model makes prediction for each instance so that the instance belongs to the class with the highest probability. Note that the denominator in the above function normalizes the probability.\n",
    "\n",
    "**Note** the Softmax Regression classifier predicts only one class at a time (its single output, i.e., you ask about the plant it answers varsicolor. But cannot recognize all the people in photo.)\n",
    "\n",
    "**Cost Function and Training**\n",
    "\n",
    "The cost function of the Softmax Regression is called **cross entropy**. It can be written as\n",
    "\n",
    "$J(\\Theta)=-\\frac{1}{m}\\sum_{i=1}^{m}\\sum_{k=1}^K y^{(i)}_k log(p_k^{(i)})$\n",
    "\n",
    "**Code of Softmax Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "  return f(*args, **kwds)\n",
      "/anaconda3/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "softmax_reg = LogisticRegression(multi_class='multinomial',solver='lbfgs', C=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Nearest Neighbor (KNN)\n",
    "\n",
    "K Nearest Neighbor is a **classification** algorithm that operates on a very simple principle. The best way to learn it is through an example. Suppose we have the heights and weights of a group of dogs and horses. Then, we want an algorithm that can predict that whether the new data point is dog or horse? (Scenario: We have features of two classes dog and horse, we want to predict that the new data point belongs to which class?)\n",
    "\n",
    "**KNN steps**\n",
    "\n",
    "1. Storing all the data\n",
    "\n",
    "2. Calculate the distance from x (new data) to all points in your data\n",
    "\n",
    "3. Predict the majority label of the k closest points\n",
    "\n",
    "**Note:** The value of K will affect what class a new point is assigned to\n",
    "\n",
    "The KNN algorithm is very simple and its training is trivial. It works with any number of classes and also it's easy to add more data to be included. We need a few parameters to adjust : **K** and **Distance Metric**.\n",
    "\n",
    "**Note:** It's important to note that the KNN model does not work really well for the categorical features and high dimensional data (many features) set.\n",
    "\n",
    "**Codes:**\n",
    "\n",
    "### Standardizing and scaling data\n",
    "\n",
    "Due to the fact that KNN algorithm works based on the calculated distance of the data point required to make predictions on it to all other data points, those variables with large scale will have much larger effect on the distance and also the results of KNN algorithm. So we need first **standardized** the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardizing and scaling the Data\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(dataframe.drop('label_column',axis=1)) # label column must be excluded from scaling\n",
    "\n",
    "scaled_feature = scaler.transform(dataframe.drop('label_column',axis=1))  # After scaling data we should transform it.\n",
    "\n",
    "new_dataframe = pd.DataFrame(scaled_feature,columns=dataframe.columns[:-1]) # columns are everything except the target column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN algorithm\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=1)  # K=1\n",
    "knn.fit(X_train,y_train)\n",
    "knn.predict(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since KNNclassifier deals with classification problems we should use the proper metrics to evaluate the performance of the model. So, we use **classification report** and **confusion matrix**. The following codes are the python codes for classification report and confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "print(classification_report(y_test,prediction))\n",
    "print(confusion_matrix(y_test,prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the most important part of choosing the best K value is to generalize the algorithm for K in range(1,40) (or some other value) and calculate the error_rate as mean(ytest != prediction).\n",
    "\n",
    "The codes are as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_rate = []\n",
    "\n",
    "# Will take some time\n",
    "for i in range(1,40):\n",
    "    \n",
    "    knn = KNeighborsClassifier(n_neighbors=i)\n",
    "    knn.fit(X_train,y_train)\n",
    "    prediction_i = knn.predict(X_test)\n",
    "    error_rate.append(np.mean(prediction_i != y_test))\n",
    "    \n",
    "    \n",
    "# Then we can plot it\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(range(1,40),error_rate,color='blue', linestyle='dashed', marker='o',\n",
    "         markerfacecolor='red', markersize=10)\n",
    "plt.title('Error Rate vs. K Value')\n",
    "plt.xlabel('K')\n",
    "plt.ylabel('Error Rate')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machines (SVM)\n",
    "\n",
    "A support vector machine (SVM) is a powerful and versatile Machine learning model, capable of performing linear or nonlinear calssification, regression, and even outlier detection. It's one of the most popular models in Machine Learning.\n",
    "\n",
    "**Linear SVM Classification**\n",
    "\n",
    "The fundamental idea behind SVM is best explained with some figures. The figure shown below is a schematic representation of a **linearly separable** data set. It's linearly separable since different classes are separated by straight lines (middel figure). The Note that all the lines in the middle figure are decision boundaries that separate the classes well but they are close to instances that model will probably not perform as well on new instance. In contrast the middle straight line in the right figure, not only separates the data into two classes but also stays away from the closest training instances as possible. We can think ot **SVM classifier** as fitting the widest possibles street (represented by the two lines that touch data points from opposite classes in the right figure). The instances that are located on the edge of the street are called **support vectors**.\n",
    "<img src='files/SVM1_pic.png'/>\n",
    "\n",
    "**Note:** SVM are sensetive to the feature scales. So, we should use **sklearn.preprocessing** and **StandardScaler** before training the SVM.\n",
    "\n",
    "We have two different types of margin classification. **hard margin classification** which does not allow a data point stand in the middle of the street or on the wrong side, and **soft margin classification** that allows margin violation. The objective in the SVM is to find the balance between keeping the street as wide as possible and limiting margin violation. One of the hyperparameters of SVM is **C**. Usually for low values of C we get more margin violation and we can decrease it by increasing hyperparameter C. However, if our SVM model is overfitting we can **regularizing** it by reducing C.\n",
    "\n",
    "**code:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that we should scale the data before using these lines of codes.\n",
    "#\n",
    "\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "svc_model = LinearSVC(loss='hinge',C=1)\n",
    "svc_model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It should be noted that we can use the SVC class as well with a linear kernel as written in the code block. However, LinearSVC is much faster than SVC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC \n",
    "\n",
    "svc_model = SVC(kernel='linear', C=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Kernel Trick**\n",
    "Is a trick to replace the dot product of the transformed data with a function that maniplates the data. This function is called **kernel function**. \n",
    "\n",
    "**SVM Regression**\n",
    "\n",
    "To use SVM for regression, the trick is to reverse the objective. Instead of trying to fit the widest margin between two classes and limiting the margin violations, SVM regression tries to fit as many instances as possible on the margin while limiting margin violations for regression. Note that the definitions of margin violation for classification and regression are **opposite**. The width of the margin is controlled by a hyperparameter **$\\epsilon$**. \n",
    "\n",
    "To tackle nonlinear regression tasks we can use **kernelized SVM** model. \n",
    "\n",
    "**Codes of linear and non-linear regression using SVM:**\n",
    "\n",
    "The equivalent of SVC class is SVR class that can be used for a particular kernel for regression tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# linear regression using SVM\n",
    "from sklearn.svm import LinearSVR   # R in SVR stands for regression\n",
    "\n",
    "svm_regression = LinearSVR(epsilon=1.5)\n",
    "svm_regression.fit(X_train,y_train)\n",
    "\n",
    "#OR\n",
    "\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "svm_regression = SVR(kernel='linear',epsilon=1.5)\n",
    "svm_regression.fit(X_train,y_train)\n",
    "\n",
    "#-------------------------------------------\n",
    "\n",
    "# Non-linear regression using SVM\n",
    "\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "svm_reg = SVR(kernel='poly', degree = 2, C=100, epsilon=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Decision Tree\n",
    "\n",
    "Like SVMs, decision trees are versatile Machine learning algorithms that can perform both classification and regression and multioutput tasks. Decision Trees are the fundamental components of random forest as well.\n",
    "\n",
    "We first start with the training and visulaizing and predicting with Decision Trees.\n",
    "\n",
    "**Codes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The code of Decision Trees for classification task\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "tree_clf = DecisionTreeClassifier(max_depth=2)\n",
    "tree_clf.fit(X,y)\n",
    "\n",
    "# To predict probabilities fo each class\n",
    "\n",
    "tree_clf.predict_probab(X_test)\n",
    "tree_clf.predict(X_test)\n",
    "\n",
    "# we can visualize the trained decision tre by the following code\n",
    "\n",
    "from sklearn.tree import export_graphviz\n",
    "\n",
    "export_graphviz(tree_clf, out_file=image_path('name.dot'), feature_names = iris.feature_names[2:]\n",
    "               , class_names = iris.target_names, rounded = True, filled = True)\n",
    "\n",
    "# we can use dot command line tool from graphviz to convert .dot file to variety of formats. \n",
    "#ex:\n",
    "\n",
    "dot -Tpng name.dot -o iris_tree.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Making predictions**\n",
    "\n",
    "In a Decision Trees graph, **node** is a block that asks questions or specifies the class of that instance. The first node is **root node**. The node that specifies class is called **leaf node**. One of the advantage of Decision Trees is that they require very little data preparation and they don't require any feature scaling or centering at all. The impurity of each node is measured by **gini impurity**. The gini impurity of the ith node can be calculated as\n",
    "\n",
    "$G_i = 1- \\sum_{k=1}^n p_{i,k}^2$\n",
    "\n",
    "where $p_{i,k}$ is the ratio of the class k instances among the training instances in the ith node. Scikit Learn uses **CART** algorithm, which produces only binary trees. Implying that nonleaf nodes always have two children nodel (Yes/No answers.) We can use other algorithms such as ID3 to produce decision trees with more than two children. \n",
    "\n",
    "**Cart Training algorithm**\n",
    "                                                                                                                 Scikit learn uses the **classification and regression tree (CART)** algorithm to train decision trees. The algorithm works by first splitting the training set into two subses using a single feature k and threshold $t_k$. It searches for the purest subsets. The following function is the **CART cost function** that the algorithm tries to minimize\n",
    "    \n",
    "    \n",
    "$J(k,t_k) = \\frac{m_{left}}{m}G_{left}+ \\frac{m_{right}}{m}G_{right}$\n",
    "\n",
    "where, $G_{left/right}$ measures the impurity of the left/right subset, and $m_{left/right}$ is the number of instances in the left/right subset. Once the CART algorithm splits the datasert into two subsets successfuly, it splits the subsets using same logic then the subsets, ans so on. It stops when it reaches the **maximum depth** or it cannot find a split that will reduce impurity.  \n",
    "\n",
    "**Gini impurity or Entropy**\n",
    "\n",
    "By default the Gini impurity measure is used for decision trees but we can select entropy impurtiy measure instead by setting the criterion hyperparameter to entropy. In Machine learning, entropy is frequently used as an impurity measure. A set's entropy is zero when it contains instances of only one class. The entropy of the ith node can be written as\n",
    "\n",
    "$H_i = - \\sum_{k=1,p_{i,k\\neq0}}^n p_{i,k}log_2(p_{i,k})$\n",
    "\n",
    "**Decision Trees for regression**\n",
    "\n",
    "Decision Trees are also capable of performing regression tasks. The following block of codes shows how to implement the Decision Trees for a regressor task:\n",
    "\n",
    "**Codes**                                                                                                               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "tree_reg = DecisionTreeRegressor(max_depth = 2)\n",
    "tree_reg.fit(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cost function that **CART** algorithm trying to minimize to find the best result for a regression problem can be written as\n",
    "\n",
    "$J(k,t_k) = \\frac{m_{left}}{m}MSE_{left}+\\frac{m_{right}}{m}MSE_{right}$\n",
    "\n",
    "where,\n",
    "\n",
    "$MSE_{node} = \\sum_{i\\in node}(y_{node}-y^{(i)})^2$ and $y_{node} = \\frac{1}{m_node}$.\n",
    "\n",
    "**Instability of Decision Tree**\n",
    "\n",
    "Although Decision Tree is one of the most capable algorithms of ML, but it has a few limitations\n",
    "\n",
    "  1- Decision Trees love orthogonal boundaries\n",
    "  \n",
    "  2- It's very sensetive to the training dataset.(This limitation can be resolved by RandomForest by averaging many trees randomly.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble learning and Random Forest\n",
    "\n",
    "Usually if we aggregate the predictions of a group of predictors (classifiers or regressors), we will often get a better predictions than with the best individual predictor. A group of predictors is called an **ensemble**. This technique is called **ensemble learning** and an ensemble learning algorithm is called an **ensemble method**.\n",
    "\n",
    "The most popular ensemble methods are:\n",
    "\n",
    "  1. Voting Classifiers\n",
    "  \n",
    "  2. Bagging (bootstrap aggregating) and Pasting\n",
    "  \n",
    "  3. Random Forests\n",
    "  \n",
    "  4. Boosting\n",
    "    - Ada Boost\n",
    "    - Gradient Boost\n",
    "    \n",
    "  5. Stacking  \n",
    "  \n",
    "**Random Forest**\n",
    "\n",
    "Random Forest is an **ensemle** of Decision Trees, generally trained via bagging method, typically with **max_sample** set to the size of **training set**. So it is a **Bagging Classifier** such that the predictors are Decision Trees. The Scikit learn code of Random Forest is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rnd_clf = RandomForestClassifier(n_estimator = 500, max_leaf_nodes=16, n_jobs=-1)\n",
    "rnd_clf.fit(X,y)\n",
    "\n",
    "# n_estimator is the number of Decision Trees used in this RandomForestClassifier\n",
    "# max_leaf_nodes The maximum number of leaf nodes\n",
    "# n_job specifies the number of CPU cores for training and n_job=-1 means all available CPU cores\n",
    "\n",
    "y_pred_rf = rnd_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note: Extra Trees**\n",
    "\n",
    "When we are growing a tree in a Random Forest, at each node only a random subset of the features is considered for slplitting . It is possible to make trees even more random by also using random threshold for each feature rather than searching for best possible thresholds. As a reminder, each node in Decision Tree asks a question regarding one of the features. If the value of that feature is greather than a value then the answer is yes and if it is less than that value the answer is no. This value is called to be **threshold**. \n",
    "\n",
    "A forest of such extremely random trees (random subset of features and random threshold for each feature), is called to be **extremely randomized trees ensemble**. This technique trades more bias  for a lower variance. Note that this technique makes extra-trees faster to train than regular RandomForest. \n",
    "\n",
    "**Feature Importance**\n",
    "\n",
    "One of the importance of Random Forests is that it can measure the relative importance of each feature. It measures the feature's importance by by looking at how much the tree nodes that use that feature reduce impurity on average (across all trees in the forest). It is a wheighted average where each node's wheight is equal to the number of training samples that are associated with it. \n",
    "\n",
    "Scikit learn computes this score automatically for each feature after training, then scales the scores so that the sum of all importance is equal to 1. We can acces the result using the **feature_importances_**. The following code shows the method using iris dataset. \n",
    "\n",
    "**code**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sepal length (cm) 0.10195334224382216\n",
      "sepal width (cm) 0.02264774416104493\n",
      "petal length (cm) 0.43246710186984005\n",
      "petal width (cm) 0.4429318117252926\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "iris = load_iris()\n",
    "rnd_fclf = RandomForestClassifier(n_estimators=500, n_jobs=-1)\n",
    "\n",
    "rnd_fclf.fit(iris['data'],iris['target'])\n",
    "for name, score in zip(iris['feature_names'], rnd_fclf.feature_importances_):\n",
    "    print(name, score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
